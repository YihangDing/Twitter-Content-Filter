{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import VGG16,ResNet50,InceptionV3, MobileNetV2\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "import urllib.request\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global counter\n",
    "counter = 1\n",
    "filelist = [ f for f in os.listdir('./text_file')]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('./text_file', f))\n",
    "filelist = [ f for f in os.listdir('./picture')]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('./picture', f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base_dir & model &weights path\n",
    "BASE_DIR = ''\n",
    "WEIGHTS_DIR = os.path.join(BASE_DIR, 'MobileNetV2_weights_5_10_fc3.h5')\n",
    "# MODEL_DIR = os.path.join(BASE_DIR, 'MobileNetV2_model_5_10.h5')\n",
    "CLASS_NAME = ['suit_adult','suit_none', 'suit_all']\n",
    "TARGET_SHAPE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compile model, load weights\n",
    "conv_base2 = MobileNetV2(weights='imagenet',include_top=False,input_shape=(TARGET_SHAPE, TARGET_SHAPE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model2 = Sequential()\n",
    "model2.add(conv_base2)\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(64, activation=tf.nn.relu))\n",
    "# model2.add(Dropout(0.4))\n",
    "model2.add(Dense(32, activation=tf.nn.relu))\n",
    "model2.add(Dense(32, activation=tf.nn.relu))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# model2.summary()\n",
    "conv_base2.trainable=False\n",
    "\n",
    "model2.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "model2.load_weights(WEIGHTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(model2, CLASS_NAME, img):\n",
    "    # predict a image, return its class name\n",
    "    img = cv2.resize(img, (TARGET_SHAPE, TARGET_SHAPE))\n",
    "    img_pred = np.expand_dims(img,axis = 0)\n",
    "    pred = np.argmax(model2.predict(img_pred))\n",
    "    return CLASS_NAME[pred], pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values below with yours\n",
    "\"\"\"\n",
    "CONSUMER_KEY = 'GpptFmNR9usFQ9AlxXenjH33C'\n",
    "CONSUMER_SECRET = 'pBVjChbX97GpLpAQG3zm7HTPP34Ui5EbZfOUmCklr0f73WgDWH'\n",
    "ACCESS_TOKEN = '715011126598078464-V72WAPYEI0m1UwjILwpYyM4EWIvFm3U'\n",
    "ACCESS_TOKEN_SECRET = 'WCIFP0Bolszk6y7GCPLJ2rZ8BsK1OdbyMWdC51ezNK8xx'\n",
    "\"\"\"\n",
    "\n",
    "CONSUMER_KEY = \"gogcVeWEFKKbL2ukNeApLAdOm\" \n",
    "CONSUMER_SECRET= \"dC10b43ZxblWn1JyRgyJ1J9HtW8BI3P6nAyF6rxnVqyMQE298C\"\n",
    "ACCESS_TOKEN = \"1119010974315503616-jMb6tt2UndlpltbptiNFvfggHWnkWm\"\n",
    "ACCESS_TOKEN_SECRET = \"NagNb7QdqTwnwpfEpleHjqKfJJYqmEKXByXpsKFN2pkUf\"\n",
    "\n",
    "my_auth = requests_oauthlib.OAuth1(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_list(list1, list2):\n",
    "    list1 = [item.lower() for item in list1]\n",
    "    list2 = [item.lower() for item in list2]\n",
    "    l1_set = set(list1)\n",
    "    for item in list2:\n",
    "        if item in l1_set:\n",
    "            return True, item\n",
    "    return False, None\n",
    "hashtags = ['pussy', 'cumslut', 'gangbang', 'nake', 'naked', 'dildo',  'webcams', \\\n",
    "           'nudes', 'freenudes', 'xxx', 'camgirl', 'amateur', \\\n",
    "           'squirt', 'nude', 'cumtribute', 'fuckme', \"porn\",\\\n",
    "           'fetish',  'fisting', 'massage', 'milf', \\\n",
    "           'bbw', 'blowjob', 'creampie', 'orgasm', \\\n",
    "           \"cum\",\"lesbian\", \"milf\", \"tit\", 'tits', \\\n",
    "           \"cock\",\"hentai\",\"anal\",\\\n",
    "           \"whore\", \"bitch\", \"lonely\", \\\n",
    "           \"nsfw\", \"sex\", \"sexy\", \"horny\", 'cosplay', 'ass', \"fuck\", \"hardcore\"]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "    url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "    query_data = [('language', 'en'), ('locations', '-130,-20,100,50'),('track','#')]\n",
    "    query_url = url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in query_data])\n",
    "    response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "    print(query_url, response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common(x, y):\n",
    "    for i in x:\n",
    "        if i in y:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "    global counter\n",
    "    time_list = []\n",
    "    time_step = 0\n",
    "    image_num = 0\n",
    "    filter_num = 0\n",
    "    for line in http_resp.iter_lines():\n",
    "        try:\n",
    "            full_tweet = json.loads(line)\n",
    "            tweet_text = full_tweet['text']\n",
    "            \n",
    "            hashtags_raw = full_tweet[\"entities\"][\"hashtags\"]\n",
    "            hashtags_got = list()\n",
    "            for hashtag in hashtags_raw:\n",
    "                hashtags_got.append(hashtag[\"text\"].lower())\n",
    "                \n",
    "            # if any hashtag match happens\n",
    "#             if not common(hashtags, hashtags_got):\n",
    "#                 continue\n",
    "            s = tweet_text.split(' ')\n",
    "            flag, data = common_list(s, hashtags)\n",
    "            if \"extended_entities\" in full_tweet:\n",
    "#             if \"extended_entities\" in full_tweet and flag == True:\n",
    "                time_list += [time.time()]\n",
    "                print('Keyword:', data)\n",
    "                if hashtags_got:\n",
    "                    print('Hashtag:', hashtags_got)\n",
    "                else:\n",
    "                    print('Hashtag: no hashtag')\n",
    "                \n",
    "                print('Tweet text:\\n', tweet_text)\n",
    "                if \"media\" in full_tweet[\"extended_entities\"]:\n",
    "                    f = open('./text_file/'+str(counter)+'.txt', 'w+')\n",
    "                    f.writelines('keyword   ')\n",
    "                    f.writelines(str(data))\n",
    "                    f.writelines('\\n')\n",
    "                    f.writelines('hashtag   ')\n",
    "                    f.writelines(str(hashtags_got))\n",
    "                    f.writelines('\\n')\n",
    "                    f.writelines('text   ')\n",
    "                    f.writelines(tweet_text)\n",
    "                    f.writelines('\\n')\n",
    "                    j = 1\n",
    "                    picture_list = []\n",
    "                    result_list = []\n",
    "                    tweet_image = []\n",
    "                    for i in range(len(full_tweet[\"extended_entities\"][\"media\"])):\n",
    "                        tweet_image += [full_tweet[\"extended_entities\"][\"media\"][i][\"media_url\"]]\n",
    "                        picture_list += [str(counter)+'_'+str(j)+'.jpg']\n",
    "                        urllib.request.urlretrieve(full_tweet[\"extended_entities\"][\"media\"][i][\"media_url\"], './picture/'+str(counter)+'_'+str(j)+'.jpg')\n",
    "                        img = cv2.imread('./picture/'+str(counter)+'_'+str(j)+'.jpg')\n",
    "                        res_class, _ = predict_img(model2, CLASS_NAME, img)\n",
    "                        result_list += [res_class]\n",
    "                        j += 1\n",
    "                        image_num += 1\n",
    "                        if res_class == 'suit_adult' or res_class == 'suit_none':\n",
    "                            filter_num += 1\n",
    "                    \n",
    "                    f.writelines('picture')\n",
    "                    print(\"Picture url:\", end = '   ')\n",
    "                    for k in range(len(picture_list)):\n",
    "                        f.writelines('   ')\n",
    "                        f.writelines(str(picture_list[k]))\n",
    "                        print(str(tweet_image[k]), end = '   ')\n",
    "                    f.writelines('\\n')\n",
    "                    print('\\n', end = '')\n",
    "                    \n",
    "                    f.writelines('result')\n",
    "                    print(\"Result:\", end = '   ')\n",
    "                    for k in range(len(result_list)):\n",
    "                        f.writelines('   ')\n",
    "                        f.writelines(str(result_list[k]))\n",
    "                        print(str(result_list[k]), end = '   ')\n",
    "                    f.writelines('\\n')\n",
    "                    print('\\n', end = '')\n",
    "                    counter += 1\n",
    "                    \n",
    "                    for i in range(time_step, len(time_list)):\n",
    "                        total_time = time.time() - time_list[i]\n",
    "                        if total_time > 10:\n",
    "                            os.remove('./text_file/' + str(i+1) + '.txt')\n",
    "                            for loopp in range(1, 5):\n",
    "                                try:\n",
    "                                    os.remove('./picture/' + str(i+1) + '_' + str(loopp) + '.jpg')\n",
    "                                except:\n",
    "                                    pass\n",
    "                        else:\n",
    "                            time_step = i\n",
    "                            break\n",
    "                    \n",
    "                    print('Total picture numbers:', image_num)\n",
    "                    print('Total filter numbers:', filter_num)\n",
    "                    print('ratio:', str(round(filter_num/image_num, 1)))\n",
    "                    f2 = open('counter.txt', 'w')\n",
    "                    f2.write(str(image_num))\n",
    "                    f2.write('\\t')\n",
    "                    f2.write(str(filter_num))\n",
    "                    f2.write('\\t')\n",
    "                    f2.write(str(round(filter_num/image_num, 1)))\n",
    "                    f2.close()\n",
    "                    \n",
    "                    print (\"end------------------------------------------\")\n",
    "                    tcp_connection.send(line)\n",
    "                else:\n",
    "                    print(\"picture url: No image\")\n",
    "                    print (\"end------------------------------------------\")\n",
    "        except:\n",
    "            raise\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected... Starting getting tweets.\")\n",
    "resp = get_tweets()\n",
    "send_tweets_to_spark(resp, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! lsof -i:9010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! kill "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
